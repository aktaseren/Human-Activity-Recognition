############################################################################################
######################### Human Activity Recognition-Classification ########################
############################################################################################
####################################Coded by Eren Aktas#####################################
############################################################################################
### Importing necessary libraries to use the related functions
.libPaths()
install.packages('e1071', dependencies = TRUE)
install.packages('party')
install.packages('randomForest')
library(class) 
library(e1071)
library(party)
library(randomForest)
library(MASS)
library(caret)

set.seed(42)

#############################################################################################
##################################### Data Understanding ####################################
#############################################################################################
# Dataset Folder downloaded from:
# https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones

### Data Integration Part
dataSources <- "UCI_HAR_Dataset"

setwd("C:/Users/AKTAS/Desktop/Dissertation/UCI_HAR_Dataset")

if(!file.exists(dataSources)){
  ttable <- read.table("activity_labels.txt", sep = "")
  activity_labels <- as.character(ttable$V2)
  ttable <- read.table("features.txt", sep = "")
  attribute_names <- ttable$V2
  
  Xtrain <- read.table("train/X_train.txt", sep = "")
  names(Xtrain) <- attribute_names
  Ytrain <- read.table("train/y_train.txt", sep = "")
  names(Ytrain) <- "Activity "
  Ytrain$Activity <- as.factor(Ytrain$Activity)
  levels(Ytrain$Activity) <- activity_labels
  train_subjects <- read.table("train/subject_train.txt", sep = "")
  names(train_subjects) <- "subject "
  train_subjects$subject <- as.factor(train_subjects$subject )
  train_data <- cbind (Xtrain, train_subjects, Ytrain )
  
  Xtest <- read.table("test/X_test.txt", sep = "")
  names(Xtest) <- attribute_names
  Ytest <- read.table("test/y_test.txt", sep = "")
  names(Ytest) <- "Activity "
  Ytest$Activity <- as.factor( Ytest$Activity )
  levels(Ytest$Activity) <- activity_labels
  test_subjects <- read.table("test/subject_test.txt", sep = "")
  names(test_subjects) <- "subject"
  test_subjects$subject <- as.factor(test_subjects$subject )
  test_data <- cbind (Xtest, test_subjects, Ytest )
  
  save(train_data, test_data, file = dataSources)
  remove(ttable, activity_labels, attribute_names, Xtrain, Ytrain,
         train_subjects, Xtest, Ytest, test_subjects)
}

### Data Combining Part
dataSources <- "UCI_HAR_Dataset"
load(dataSources)

test_data <- test_data[,-563]
train_data <- train_data[,-c(562,564)]
HAR <- rbind(train_data, test_data)

remove(train_data, test_data, dataSources)

############################################################################################
######################## Data Exploration & Preparation ####################################
############################################################################################
dim(HAR) #Dimensions

colnames(HAR) #Feature Names

summary(HAR) #Structure of Dataset

attributes(HAR) #Attributes

View(HAR[,c(562,563)]) #Subject and Activity Features
View(HAR[c(1:3),]) # The first 3 records


## Ordering levels of subject feature owing to its disorder
HAR$subject <- factor(HAR$subject,levels = c("1", "2", "3", "4","5","6","7","8","9","10",
                                             "11","12","13","14","15","16","17","18","19",
                                             "20","21","22","23","24","25","26","27","28",
                                             "29","30"))

## Making illegal Feature Names Syntactically Valid
names(HAR) <- make.names(names(HAR))

# The control of Missing Values
sum(is.na(HAR))

# Frequency of Target Variable
table(HAR$Activity)
barplot(table(HAR$Activity),col= c("green","red","black","blue", "yellow","brown"),
        ylim=c(0,2000), cex.names=0.55, space=02)
grid()

# Frequency of 'Subject' Variable
# install.packages('ggplot2')
# install.packages('ggpubr')
library(ggplot2)
library(ggpubr)
theme_set(theme_pubr())

table(HAR$subject)
ggplot(HAR, aes(subject)) +
  geom_bar(fill = "#0073C2FF") +
  ylim(0,500) +
  theme_pubclean()

## To use the dataset for tools like Tableau, we need to export it by following code:
## 1st option:
# write.table(HAR, file = "HARtable.txt")

## 2nd option:
# install.packages("openxlsx")
# library(openxlsx)
# write.xlsx(HAR,"HAR.xlsx",sheetName = "Sheet1",col.names = TRUE,showNA=FALSE)

# To see normalization over the range of scaling dataset
max(HAR[,1:561], na.rm = FALSE)
min(HAR[,1:561], na.rm = FALSE)

############################################################################################
##################################### Data Modelling #######################################
############################################################################################

############################################################################################
######################################Sampling 90:10########################################
############################################################################################
vect1=rep(0,5)
for(i in 1:5) {
  n=nrow(HAR) 
  indexes = sample(n,n*(90/100))
  trainset = HAR[indexes,]
  testset = HAR[-indexes,]
  
  svm_lin <- svm(Activity~ ., data=trainset, method='C-classification', kernel='linear')
  bnc <- naiveBayes(Activity~ ., data=trainset)
  ctree <- ctree(Activity ~ ., data=trainset)
  knn9 <- knn(train = trainset[,1:561], test = testset[,1:561], cl = trainset$Activity, k=9)
  rf <- randomForest(Activity ~ ., data = trainset)

  #test set predictions for linear
  pred_test_svm_lin <-predict(svm_lin,testset)
  p_lin= mean(pred_test_svm_lin==testset$Activity)
  
  #test set predictions for bnc
  pred_test_bnc <-predict(bnc,testset)
  p_bnc= mean(pred_test_bnc==testset$Activity)
  
  #test set predictions for decision trees
  pred_test_ctree <-predict(ctree, testset)
  p_ctree=mean(pred_test_ctree==testset$Activity)
  
  #test set predictions for kNN
  p_knn9= mean(knn9==testset$Activity)
  
  #test set predictions for Random Forest
  pred_test_rf <-predict(rf,testset)
  p_rf= mean(pred_test_rf==testset$Activity)
  
  p_vect=c(p_lin, p_bnc, p_ctree, p_knn9, p_rf)
  
  vect1 <- vect1 + (1/5)*p_vect
  
}
vect1

############################################################################################
######################################Sampling 80:20########################################
############################################################################################
vect2=rep(0,5)
for(i in 1:5) {
  
n=nrow(HAR) 
indexes = sample(n,n*(80/100))
trainset = HAR[indexes,]
testset = HAR[-indexes,]

svm_lin <- svm(Activity~ ., data=trainset, method='C-classification', kernel='linear')
bnc <- naiveBayes(Activity~ ., data=trainset)
ctree <- ctree(Activity ~ ., data=trainset)
knn9 <- knn(train = trainset[,1:561], test = testset[,1:561], cl = trainset$Activity, k=9)
rf <- randomForest(Activity ~ ., data = trainset)

#test set predictions for linear
pred_test_svm_lin <-predict(svm_lin,testset)
#confusion_matrixlin= table(pred = pred_test, true = testset$activity)
p_lin= mean(pred_test_svm_lin==testset$Activity)

#test set predictions for bnc
pred_test_bnc <-predict(bnc,testset)
#confusion_matrixlin= table(pred = pred_test, true = testset$activity)
p_bnc= mean(pred_test_bnc==testset$Activity)

#test set predictions for decision trees
pred_test_ctree <-predict(ctree, testset)
#confusion_matrix_ctree= table(pred = pred_test_ctree, true = testset$activity)
p_ctree=mean(pred_test_ctree==testset$Activity)

#test set predictions for kNN
p_knn9= mean(knn9==testset$Activity)

#test set predictions for Random Forest
pred_test_rf <-predict(rf,testset)
#confusion_matrixlin= table(pred = pred_test, true = testset$activity)
p_rf= mean(pred_test_rf==testset$Activity)

p_vect=c(p_lin, p_bnc, p_ctree, p_knn9, p_rf)

vect2 <- vect2 + (1/5)*p_vect

}
vect2

############################################################################################
######################################Sampling 70:30########################################
############################################################################################
vect3=rep(0,5)
for(i in 1:5) {
  
  n=nrow(HAR) 
  indexes = sample(n,n*(70/100))
  trainset = HAR[indexes,]
  testset = HAR[-indexes,]
  
  svm_lin <- svm(Activity~ ., data=trainset, method='C-classification', kernel='linear')
  bnc <- naiveBayes(Activity~ ., data=trainset)
  ctree <- ctree(Activity ~ ., data=trainset)
  knn9 <- knn(train = trainset[,1:561], test = testset[,1:561], cl = trainset$Activity, k=9)
  rf <- randomForest(Activity ~ ., data = trainset)
  
  #test set predictions for linear
  pred_test_svm_lin <-predict(svm_lin,testset)
  #confusion_matrixlin= table(pred = pred_test, true = testset$activity)
  p_lin= mean(pred_test_svm_lin==testset$Activity)
  
  #test set predictions for bnc
  pred_test_bnc <-predict(bnc,testset)
  #confusion_matrixlin= table(pred = pred_test, true = testset$activity)
  p_bnc= mean(pred_test_bnc==testset$Activity)
  
  #test set predictions for decision trees
  pred_test_ctree <-predict(ctree, testset)
  #confusion_matrix_ctree= table(pred = pred_test_ctree, true = testset$activity)
  p_ctree=mean(pred_test_ctree==testset$Activity)
  
  #test set predictions for kNN
  p_knn9= mean(knn9==testset$Activity)
  
  #test set predictions for Random Forest
  pred_test_rf <-predict(rf,testset)
  #confusion_matrixlin= table(pred = pred_test, true = testset$activity)
  p_rf= mean(pred_test_rf==testset$Activity)
  
  p_vect=c(p_lin, p_bnc, p_ctree, p_knn9, p_rf)
  
  vect3 <- vect3 + (1/5)*p_vect
  
}
vect3

############################################################################################
######################################Sampling 60:40########################################
############################################################################################
vect4=rep(0,5)
for(i in 1:5) {
  
  n=nrow(HAR) 
  indexes = sample(n,n*(60/100))
  trainset = HAR[indexes,]
  testset = HAR[-indexes,]
  
  svm_lin <- svm(Activity~ ., data=trainset, method='C-classification', kernel='linear')
  bnc <- naiveBayes(Activity~ ., data=trainset)
  ctree <- ctree(Activity ~ ., data=trainset)
  knn9 <- knn(train = trainset[,1:561], test = testset[,1:561], cl = trainset$Activity, k=9)
  rf <- randomForest(Activity ~ ., data = trainset)
  
  #test set predictions for linear
  pred_test_svm_lin <-predict(svm_lin,testset)
  #confusion_matrixlin= table(pred = pred_test, true = testset$activity)
  p_lin= mean(pred_test_svm_lin==testset$Activity)
  
  #test set predictions for bnc
  pred_test_bnc <-predict(bnc,testset)
  #confusion_matrixlin= table(pred = pred_test, true = testset$activity)
  p_bnc= mean(pred_test_bnc==testset$Activity)
  
  #test set predictions for decision trees
  pred_test_ctree <-predict(ctree, testset)
  #confusion_matrix_ctree= table(pred = pred_test_ctree, true = testset$activity)
  p_ctree=mean(pred_test_ctree==testset$Activity)
  
  #test set predictions for kNN
  p_knn9= mean(knn9==testset$Activity)
  
  #test set predictions for Random Forest
  pred_test_rf <-predict(rf,testset)
  #confusion_matrixlin= table(pred = pred_test, true = testset$activity)
  p_rf= mean(pred_test_rf==testset$Activity)
  
  p_vect=c(p_lin, p_bnc, p_ctree, p_knn9, p_rf)
  
  vect4 <- vect4 + (1/5)*p_vect
  
}
vect4

############################################################################################
############################################################################################
############################################################################################
Mat=cbind(vect1,vect2,vect3,vect4)

xdata <- c(60,70,80,90)
y1 <- Mat[1,]
y2 <- Mat[2,] 
y3 <- Mat[3,]
y4 <- Mat[4,] 
y5 <- Mat[5,]

# plot the first curve by calling plot() function
# First curve is plotted
plot(xdata, y1, xlab="Split Ratio", ylab="Probability of Correctness", type="o", col="blue",
     pch="o", lty=1,  ylim=c(0.5,1) )

points(xdata, y2, col="red", pch="*")
lines(xdata, y2, col="red",lty=2)

points(xdata, y3, col="green",pch="+")
lines(xdata, y3, col="green", lty=3)

points(xdata, y4, col="black",pch="*")
lines(xdata, y4, col="black", lty=3)


points(xdata, y5, col="purple",pch="o")
lines(xdata, y5, col="purple", lty=3)

legend(
  "bottomright", 
  lty=c(1,2,1,2), cex=0.55,
  col=c("blue", "red","green", "black", "purple"), 
  legend = c("Linear_SVM", "BNC", "DT", "KNN","RF")
)

############################################################################################
############################################################################################
#The Best Accuracies of algorithms belong to the 90:10 split ratio
#90:10 Split Ratio Confusion Matrix Analysis

library(caret)
caret::confusionMatrix(pred_test_svm_lin, testset$Activity, positive="1", mode="everything")
caret::confusionMatrix(pred_test_bnc, testset$Activity, positive="1", mode="everything")
caret::confusionMatrix(pred_test_ctree, testset$Activity, positive="1", mode="everything")
caret::confusionMatrix(knn9, testset$Activity, positive="1", mode="everything")
caret::confusionMatrix(pred_test_rf, testset$Activity, positive="1", mode="everything")



############################################################################################
############################################################################################
############################################################################################
############################################################################################
############################################################################################
############################################################################################
############################################################################################
############################################################################################
############################################################################################
############################################################################################
############################################################################################
